{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03a3b101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RepeatedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, recall_score, precision_score, confusion_matrix, mean_absolute_percentage_error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2804e188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    airline   flight source_city departure_time stops   arrival_time  \\\n",
      "0  SpiceJet  SG-8709       Delhi        Evening  zero          Night   \n",
      "1  SpiceJet  SG-8157       Delhi  Early_Morning  zero        Morning   \n",
      "2   AirAsia   I5-764       Delhi  Early_Morning  zero  Early_Morning   \n",
      "3   Vistara   UK-995       Delhi        Morning  zero      Afternoon   \n",
      "4   Vistara   UK-963       Delhi        Morning  zero        Morning   \n",
      "\n",
      "  destination_city    class  duration  days_left  price  airline_label  \\\n",
      "0           Mumbai  Economy      2.17          1   5953              4   \n",
      "1           Mumbai  Economy      2.33          1   5953              4   \n",
      "2           Mumbai  Economy      2.17          1   5956              0   \n",
      "3           Mumbai  Economy      2.25          1   5955              5   \n",
      "4           Mumbai  Economy      2.33          1   5955              5   \n",
      "\n",
      "   source_city_label  destination_city_label  departure_time_label  \\\n",
      "0                  2                       5                     3   \n",
      "1                  2                       5                     0   \n",
      "2                  2                       5                     0   \n",
      "3                  2                       5                     1   \n",
      "4                  2                       5                     1   \n",
      "\n",
      "   arrival_time_label  stops_label  \n",
      "0                   4            0  \n",
      "1                   1            0  \n",
      "2                   0            0  \n",
      "3                   2            0  \n",
      "4                   1            0  \n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv ('/Users/nina/Downloads/mlproject/data/Clean_Dataset.csv')\n",
    "# drop index column and check the datatype\n",
    "data = data.drop(['Unnamed: 0'], axis=1)\n",
    "# label encode three categorical columns\n",
    "le = LabelEncoder()\n",
    "data[\"airline_label\"] = le.fit_transform(data['airline'])\n",
    "data[\"source_city_label\"] = le.fit_transform(data['source_city'])\n",
    "data[\"destination_city_label\"] = le.fit_transform(data['destination_city'])\n",
    "# category time and stops according to sequence\n",
    "\n",
    "def time_label(value):\n",
    "    if value == \"Early_Morning\":\n",
    "        return 0\n",
    "    elif value == \"Morning\":\n",
    "        return 1\n",
    "    elif value == \"Afternoon\":\n",
    "        return 2\n",
    "    elif value == \"Evening\":\n",
    "        return 3\n",
    "    elif value == \"Night\":\n",
    "        return 4\n",
    "    elif value == \"Late_Night\":\n",
    "        return 5\n",
    "\n",
    "def stops_label(value):\n",
    "    if value == \"zero\":\n",
    "        return 0\n",
    "    elif value == \"one\":\n",
    "        return 1\n",
    "    elif value == \"two_or_more\":\n",
    "        return 2\n",
    "    \n",
    "\n",
    "data['departure_time_label'] = data['departure_time'].map(time_label)\n",
    "data['arrival_time_label'] = data['arrival_time'].map(time_label)\n",
    "data['stops_label'] = data['stops'].map(stops_label)\n",
    "\n",
    "# Split Dataframe using groupby()\n",
    "# grouping by economy and business class\n",
    "data['class_label'] = np.where(data['class'] == \"Economy\", True, False)\n",
    "grouped = data.groupby(data.class_label)\n",
    "economyData = grouped.get_group(True)\n",
    "economyData=economyData.drop(['class_label'],axis=1)\n",
    "print(economyData.head())\n",
    "data=economyData.drop(['airline', 'flight', 'source_city','departure_time','stops','arrival_time', 'destination_city','class'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20563fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   duration  days_left  airline_label  source_city_label  \\\n",
      "0      2.17          1              4                  2   \n",
      "1      2.33          1              4                  2   \n",
      "2      2.17          1              0                  2   \n",
      "3      2.25          1              5                  2   \n",
      "4      2.33          1              5                  2   \n",
      "\n",
      "   destination_city_label  departure_time_label  arrival_time_label  \\\n",
      "0                       5                     3                   4   \n",
      "1                       5                     0                   1   \n",
      "2                       5                     0                   0   \n",
      "3                       5                     1                   2   \n",
      "4                       5                     1                   1   \n",
      "\n",
      "   stops_label  \n",
      "0            0  \n",
      "1            0  \n",
      "2            0  \n",
      "3            0  \n",
      "4            0  \n",
      "   price\n",
      "0   5953\n",
      "1   5953\n",
      "2   5956\n",
      "3   5955\n",
      "4   5955\n"
     ]
    }
   ],
   "source": [
    "X=data.drop(['price'],axis=1)\n",
    "y=data[['price']]\n",
    "Train_X, Test_X, Train_Y, Test_Y = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state = 42)\n",
    "rav_train_Y = np.ravel(Train_Y)\n",
    "rav_test_Y = np.ravel(Test_Y)\n",
    "print(X.head())\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be584133",
   "metadata": {},
   "source": [
    "# models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9989906",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b143091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative MAPE:  -0.369246700299228\n",
      "Best param are : {'alpha': 0.99, 'max_iter': 500}\n"
     ]
    }
   ],
   "source": [
    "Ridge_model = Ridge()\n",
    "# define Ridge_grid\n",
    "Ridge_grid = dict()\n",
    "##tune alpha\n",
    "Ridge_grid['alpha'] = np.arange(0.01, 1, 0.01)\n",
    "Ridge_grid['max_iter'] = np.array([500,1000,2000])\n",
    "Ridge_search = GridSearchCV(Ridge_model, Ridge_grid, scoring='neg_mean_absolute_percentage_error', cv=cv, n_jobs=-1)\n",
    "Ridge_results = Ridge_search.fit(Train_X, Train_Y)\n",
    "print('Negative MAPE: ' , Ridge_results.best_score_)\n",
    "print('Best param are : %s' % Ridge_results.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a019d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative MAPE:  -0.3692240875097642\n",
      "Best params are : {'alpha': 0.99, 'max_iter': 500}\n"
     ]
    }
   ],
   "source": [
    "Lasso_model = Lasso()\n",
    "# define Lasso_grid\n",
    "Lasso_grid = dict()\n",
    "##tune alpha\n",
    "Lasso_grid['alpha'] = np.arange(0.01, 1, 0.01)\n",
    "Lasso_grid['max_iter'] = np.array([500,1000,2000])\n",
    "Lasso_search = GridSearchCV(Lasso_model, Lasso_grid, scoring='neg_mean_absolute_percentage_error', cv=cv, n_jobs=-1)\n",
    "Lasso_results = Lasso_search.fit(Train_X, Train_Y)\n",
    "print('Negative MAPE: ' ,Lasso_results.best_score_)\n",
    "print('Best params are : %s' % Lasso_results.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83fe8608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative MAPE:  -0.3683633311842456\n",
      "Best params are : {'alpha': 0.08, 'max_iter': 500}\n"
     ]
    }
   ],
   "source": [
    "ElasticNet_model = ElasticNet()\n",
    "ElasticNet_cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# define ElasticNet_grid\n",
    "ElasticNet_grid = dict()\n",
    "##tune alpha\n",
    "ElasticNet_grid['alpha'] = np.arange(0.01, 1, 0.01)\n",
    "ElasticNet_grid['max_iter'] = np.array([500,1000,2000])\n",
    "ElasticNet_search = GridSearchCV(ElasticNet_model, ElasticNet_grid, scoring='neg_mean_absolute_percentage_error', cv=cv, n_jobs=-1)\n",
    "ElasticNet_results = ElasticNet_search.fit(Train_X,Train_Y)\n",
    "print('Negative MAPE: ' , ElasticNet_results.best_score_)\n",
    "print('Best params are : %s'% ElasticNet_results.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64181aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative MAPE: -0.16828584855564635\n",
      "Best params are : {'max_depth': 25, 'max_leaf_nodes': 1000}\n"
     ]
    }
   ],
   "source": [
    "DecisionTree_model = DecisionTreeRegressor()\n",
    "DecisionTree_cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# define DecisionTree_grid\n",
    "DecisionTree_grid = dict()\n",
    "##tune alpha\n",
    "DecisionTree_grid['max_depth'] = np.arange(5, 30, 5)\n",
    "DecisionTree_grid['max_leaf_nodes'] = np.array([10,100,1000])\n",
    "DecisionTree_search = GridSearchCV(DecisionTree_model, DecisionTree_grid, scoring='neg_mean_absolute_percentage_error', cv=cv, n_jobs=-1)\n",
    "DecisionTree_results = DecisionTree_search.fit(Train_X, Train_Y)\n",
    "print('Negative MAPE:' , DecisionTree_results.best_score_)\n",
    "print('Best params are : %s'% DecisionTree_results.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca712bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative MAPE: -0.1630026039870373\n",
      "Best params are : {'max_depth': 15, 'max_leaf_nodes': 1000}\n"
     ]
    }
   ],
   "source": [
    "#Random forest regression\n",
    "RandomForest_model = RandomForestRegressor()\n",
    "RandomForest_cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# define RandomForest_grid\n",
    "RandomForest_grid = dict()\n",
    "##tune alpha\n",
    "RandomForest_grid['max_depth'] = np.arange(5, 30, 5)\n",
    "RandomForest_grid['max_leaf_nodes'] = np.array([10,100,1000])\n",
    "# RandomForest_grid['n_estimators'] = np.array([50,100,200])\n",
    "\n",
    "RandomForest_search = GridSearchCV(RandomForest_model, RandomForest_grid, scoring='neg_mean_absolute_percentage_error', cv=cv, n_jobs=-1)\n",
    "RandomForest_results = RandomForest_search.fit(Train_X, rav_train_Y)\n",
    "print('Negative MAPE:', RandomForest_results.best_score_)\n",
    "print('Best params are : %s'% RandomForest_results.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "113cb284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative MAPE: -0.3692387796853123\n",
      "Best params are : {'alpha_1': 0.01, 'lambda_1': 1.0, 'lambda_2': 1e-06, 'n_iter': 200}\n"
     ]
    }
   ],
   "source": [
    "BayesianRidge_model = BayesianRidge()\n",
    "BayesianRidge_cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "BayesianRidge_grid = dict()\n",
    "\n",
    "\n",
    "BayesianRidge_grid['alpha_1'] = np.arange(0.01, 1, 0.01)\n",
    "BayesianRidge_grid['n_iter'] = np.array([200,400,600])\n",
    "BayesianRidge_grid['lambda_1'] = np.array([1e-6, 1e-3, 1])\n",
    "BayesianRidge_grid['lambda_2'] = np.array([1e-6, 1e-3, 1])\n",
    "\n",
    "\n",
    "BayesianRidge_search = GridSearchCV(BayesianRidge_model, BayesianRidge_grid, scoring='neg_mean_absolute_percentage_error', cv=cv, n_jobs=-1)\n",
    "BayesianRidge_results = BayesianRidge_search.fit(Train_X, rav_train_Y)\n",
    "print('Negative MAPE:' , BayesianRidge_results.best_score_)\n",
    "print('Best params are : %s'% BayesianRidge_results.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "550e9d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative MAPE: -0.2893346208831779\n",
      "Best params are : {'learning_rate': 0.8, 'n_estimators': 1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "adaBoost = AdaBoostRegressor()\n",
    "adaBoost_cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "adaBoost_grid = dict()\n",
    "adaBoost_grid['learning_rate'] = np.arange(0.1, 1, 0.1)\n",
    "adaBoost_grid['n_estimators'] = np.array([1,50,10])\n",
    "\n",
    "adaBoost_search = GridSearchCV(adaBoost, adaBoost_grid, scoring='neg_mean_absolute_percentage_error', cv=cv, n_jobs=-1)\n",
    "adaBoost_results = adaBoost_search.fit(Train_X, rav_train_Y)\n",
    "print('Negative MAPE:' , adaBoost_results.best_score_)\n",
    "print('Best params are : %s'% adaBoost_results.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b153da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative MAPE: -0.09613776726474814\n",
      "Best params are : {'n_estimators': 4}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "bagging = BaggingRegressor()\n",
    "bagging_grid = dict()\n",
    "bagging_grid['n_estimators'] = np.arange(1, 5, 1)\n",
    "\n",
    "bagging_search = GridSearchCV(bagging, bagging_grid, scoring='neg_mean_absolute_percentage_error', cv=cv, n_jobs=-1)\n",
    "bagging_results = bagging_search.fit(Train_X, rav_train_Y)\n",
    "print('Negative MAPE:' , bagging_results.best_score_)\n",
    "print('Best params are : %s'% bagging_results.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "353e59c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nina/opt/anaconda3/envs/tensorflow/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py:702: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative MAPE: -0.0973132237104423\n",
      "Best params are : {'max_depth': 30, 'n_estimators': 85}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "exTree = ExtraTreesRegressor()\n",
    "exTree_grid = dict()\n",
    "exTree_grid['n_estimators'] = np.arange(65, 95, 10)\n",
    "exTree_grid['max_depth'] = np.arange(20, 50, 5)\n",
    "\n",
    "exTree_search = GridSearchCV(exTree, exTree_grid, scoring='neg_mean_absolute_percentage_error', cv=cv, n_jobs=-1)\n",
    "exTree_results = exTree_search.fit(Train_X, rav_train_Y)\n",
    "print('Negative MAPE:' , exTree_results.best_score_)\n",
    "print('Best params are : %s'% exTree_results.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03cfc7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative MAPE: -0.09555962228834022\n",
      "Best params are : {'eta': 0.21000000000000002, 'max_depth': 15}\n"
     ]
    }
   ],
   "source": [
    "# xgboost,lightGBM,Catboost\n",
    "import xgboost as xgb \n",
    "xg_reg = xgb.XGBRegressor()\n",
    "xg_reg_cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "xg_reg_grid = dict()\n",
    "xg_reg_grid['max_depth'] = np.arange(5, 20, 5)\n",
    "xg_reg_grid['eta'] = np.arange(0.01, 0.25, 0.05)\n",
    "\n",
    "xg_reg_search = GridSearchCV(xg_reg, xg_reg_grid, scoring='neg_mean_absolute_percentage_error', cv=cv, n_jobs=-1)\n",
    "xg_reg_results = xg_reg_search.fit(Train_X, rav_train_Y)\n",
    "print('Negative MAPE:' , xg_reg_results.best_score_)\n",
    "print('Best params are : %s'% xg_reg_results.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbccc80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
